{"cells":[{"cell_type":"markdown","metadata":{},"source":["### This is the training notebook for Zephyr-beta-GPTQ finetuning with Lora for Medical text Translation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -q bitsandbytes transformers peft accelerate trl"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -q optimum auto-gptq"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T20:25:41.115887Z","iopub.status.busy":"2024-06-19T20:25:41.115486Z","iopub.status.idle":"2024-06-19T20:25:41.123063Z","shell.execute_reply":"2024-06-19T20:25:41.122151Z","shell.execute_reply.started":"2024-06-19T20:25:41.115858Z"},"trusted":true},"outputs":[],"source":["from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n","    GPTQConfig,\n","    GenerationConfig\n",")\n","from peft import (\n","    LoraConfig,\n","    PeftModel,\n","    prepare_model_for_kbit_training,\n","    get_peft_model,\n","    AutoPeftModelForCausalLM\n",")\n","import os, torch, wandb\n","from datasets import load_dataset\n","from trl import SFTTrainer"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T20:25:41.611800Z","iopub.status.busy":"2024-06-19T20:25:41.611488Z","iopub.status.idle":"2024-06-19T20:25:41.618101Z","shell.execute_reply":"2024-06-19T20:25:41.616989Z","shell.execute_reply.started":"2024-06-19T20:25:41.611776Z"},"trusted":true},"outputs":[],"source":["base_model = \"TheBloke/zephyr-7B-beta-GPTQ\"\n","dataset_name = \"cbasu/Med-EASi\""]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T20:25:42.117509Z","iopub.status.busy":"2024-06-19T20:25:42.117217Z","iopub.status.idle":"2024-06-19T20:25:43.172380Z","shell.execute_reply":"2024-06-19T20:25:43.171254Z","shell.execute_reply.started":"2024-06-19T20:25:42.117484Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['Expert', 'Simple', 'Annotation', 'sim', 'sentence_sim', 'compression', 'expert_fk_grade', 'expert_ari', 'layman_fk_grade', 'layman_ari', 'umls_expert', 'umls_layman', 'expert_terms', 'layman_terms', 'idx'],\n","        num_rows: 1397\n","    })\n","    validation: Dataset({\n","        features: ['Expert', 'Simple', 'Annotation', 'sim', 'sentence_sim', 'compression', 'expert_fk_grade', 'expert_ari', 'layman_fk_grade', 'layman_ari', 'umls_expert', 'umls_layman', 'expert_terms', 'layman_terms', 'idx'],\n","        num_rows: 196\n","    })\n","    test: Dataset({\n","        features: ['Expert', 'Simple', 'Annotation', 'sim', 'sentence_sim', 'compression', 'expert_fk_grade', 'expert_ari', 'layman_fk_grade', 'layman_ari', 'umls_expert', 'umls_layman', 'expert_terms', 'layman_terms', 'idx'],\n","        num_rows: 300\n","    })\n","})"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["dataset = load_dataset(dataset_name)\n","\n","dataset"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T20:25:43.174524Z","iopub.status.busy":"2024-06-19T20:25:43.174182Z","iopub.status.idle":"2024-06-19T20:25:43.181382Z","shell.execute_reply":"2024-06-19T20:25:43.180065Z","shell.execute_reply.started":"2024-06-19T20:25:43.174491Z"},"trusted":true},"outputs":[],"source":["def format_prompt(sample):\n","    medical_text = sample['Expert']\n","    simple_text = sample['Simple']\n","    \n","    formatted_prompt = f\"\"\"\n","    Please simplify the following medical summary so that it is easily understandable.\n","    Ensure that the key information is retained, but use simpler language and explanations.\n","    Avoid overly technical jargon and aim for clarity and readability.\n","    \n","    <medical_text>\n","    {medical_text}\n","    <medical_text/>\n","    \n","    <simple_text>\n","    {simple_text}\n","    <simple_text/>\n","    \"\"\"\n","    \n","    sample[\"text\"] = formatted_prompt\n","    \n","    return sample"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T20:25:43.183074Z","iopub.status.busy":"2024-06-19T20:25:43.182684Z","iopub.status.idle":"2024-06-19T20:25:43.210193Z","shell.execute_reply":"2024-06-19T20:25:43.209279Z","shell.execute_reply.started":"2024-06-19T20:25:43.183010Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"\\n    Please simplify the following medical summary so that it is easily understandable.\\n    Ensure that the key information is retained, but use simpler language and explanations.\\n    Avoid overly technical jargon and aim for clarity and readability.\\n    \\n    <medical_text>\\n    75-90 % of the affected people have mild intellectual disability.\\n    <medical_text/>\\n    \\n    <simple_text>\\n    People with syndromic intellectual disabilities may have a `` typical look. ''\\n    <simple_text/>\\n    \""]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["dataset = dataset.map(\n","    format_prompt\n",")\n","dataset[\"train\"][\"text\"][0]"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T20:25:43.421977Z","iopub.status.busy":"2024-06-19T20:25:43.421516Z","iopub.status.idle":"2024-06-19T20:25:48.488788Z","shell.execute_reply":"2024-06-19T20:25:48.487516Z","shell.execute_reply.started":"2024-06-19T20:25:43.421955Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n","Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n","/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:167: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n","  warnings.warn(warning_msg)\n","/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4481: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n","  warnings.warn(\n","Some weights of the model checkpoint at TheBloke/zephyr-7B-beta-GPTQ were not used when initializing MistralForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']\n","- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n","tokenizer.padding_side = 'right'\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.add_eos_token = True\n","tokenizer.add_bos_token, tokenizer.add_eos_token\n","\n","bnb_config = GPTQConfig(bits=4,\n","                        disable_exllama=True,\n","                        device_map=\"auto\",\n","                        use_cache=False,\n","                        lora_r=16,\n","                        lora_alpha=16,\n","                        tokenizer=tokenizer\n","                                )\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","        base_model,\n","        quantization_config=bnb_config,\n","        torch_dtype=torch.bfloat16,\n","        device_map=\"auto\",\n","        trust_remote_code=True,\n",")\n","\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","model.gradient_checkpointing_enable()\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T20:25:48.491030Z","iopub.status.busy":"2024-06-19T20:25:48.490675Z","iopub.status.idle":"2024-06-19T20:25:48.696491Z","shell.execute_reply":"2024-06-19T20:25:48.695417Z","shell.execute_reply.started":"2024-06-19T20:25:48.490998Z"},"trusted":true},"outputs":[],"source":["peft_config = LoraConfig(\n","                            r=16,\n","                            lora_alpha=16,\n","                            lora_dropout=0.05,\n","                            bias=\"none\",\n","                            task_type=\"CAUSAL_LM\",\n","                            target_modules=[\"q_proj\", \"v_proj\"]\n","                        )\n","model = get_peft_model(model, peft_config)"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T20:25:48.698257Z","iopub.status.busy":"2024-06-19T20:25:48.697848Z","iopub.status.idle":"2024-06-19T20:25:48.740214Z","shell.execute_reply":"2024-06-19T20:25:48.739150Z","shell.execute_reply.started":"2024-06-19T20:25:48.698220Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}],"source":["\n","OUTPUT_DIR='med-zephyr-beta'\n","OPTIMIZER = \"paged_adamw_32bit\"\n","\n","training_arguments = TrainingArguments(\n","                                        output_dir='/kaggle/working/',\n","                                        per_device_train_batch_size=8,\n","                                        gradient_accumulation_steps=1,\n","                                        optim=OPTIMIZER,\n","                                        learning_rate=2e-4,\n","                                        lr_scheduler_type=\"cosine\",\n","                                        save_strategy=\"epoch\",\n","                                        logging_steps=50,\n","                                        num_train_epochs=1,\n","                                        max_steps=100,\n","                                        fp16=True,\n","                                        evaluation_strategy=\"steps\",\n","                                        eval_steps=50,\n","                                        push_to_hub=False,\n",")"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T20:39:15.441302Z","iopub.status.busy":"2024-06-19T20:39:15.440944Z","iopub.status.idle":"2024-06-19T20:39:15.835699Z","shell.execute_reply":"2024-06-19T20:39:15.834732Z","shell.execute_reply.started":"2024-06-19T20:39:15.441276Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d14f76c375b4400b21b448b65c4e15f","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1397 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["max_steps is given, it will override any value given in num_train_epochs\n"]}],"source":["trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset['train'],\n","    eval_dataset=dataset['validation'],\n","    peft_config=peft_config,\n","    max_seq_length= 512,\n","    dataset_text_field=\"text\",\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing= False,\n",")"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-06-19T20:39:16.636866Z","iopub.status.busy":"2024-06-19T20:39:16.636527Z","iopub.status.idle":"2024-06-19T20:54:19.001610Z","shell.execute_reply":"2024-06-19T20:54:19.000686Z","shell.execute_reply.started":"2024-06-19T20:39:16.636839Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 14:55, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>0.650000</td>\n","      <td>0.763059</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.663700</td>\n","      <td>0.750094</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","max_steps is given, it will override any value given in num_train_epochs\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"text/plain":["TrainOutput(global_step=100, training_loss=0.6568189239501954, metrics={'train_runtime': 900.8879, 'train_samples_per_second': 0.888, 'train_steps_per_second': 0.111, 'total_flos': 148496743858176.0, 'train_loss': 0.6568189239501954, 'epoch': 0.5714285714285714})"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#data preprocessing for the sample input data\n","def input_data_preprocessing(example):\n","\n","    processed_example = \"<|system|>\\n You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\\n<|user|>\\n\" + example[\"instruction\"] + \"\\n<|assistant|>\\n\"\n","\n","    return processed_example\n","input_string = input_data_preprocessing(\n","    {\n","        \"instruction\": \"Most strabismus is caused by Refractive error; Muscle imbalance.\",\n","    }\n",")\n","\n","model = AutoPeftModelForCausalLM.from_pretrained(\n","    \"/content/zephyr-finetuning/checkpoint-100\",\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=\"cuda\")\n","\n","inputs = tokenizer(input_string, return_tensors=\"pt\").to(\"cuda\")\n","\n","generation_config = GenerationConfig(\n","    do_sample=True,\n","    top_k=1,\n","    temperature=0.1,\n","    max_new_tokens=256,\n","    pad_token_id=tokenizer.eos_token_id\n",")\n","\n","outputs = model.generate(**inputs, generation_config=generation_config)\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
